import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
import os

# Cáº¥u hÃ¬nh Ä‘Æ°á»ng dáº«n
INDEX = "data/rfc_index.faiss"
CHUNKS = "data/rfc_chunks.npy"
MAP = "data/rfc_map.txt"

# 1. Load Model
print("Báº¯t Ä‘áº§u load model...")
model = SentenceTransformer("all-MiniLM-L6-v2")
print("âœ” Model loaded!")

# 2. Load Data (Index, Chunks, Mapping)
# Báº¡n cáº§n load Chunks dÃ¹ Index cÃ³ load Ä‘Æ°á»£c hay khÃ´ng, vÃ¬ hÃ m search cáº§n dÃ¹ng nÃ³ Ä‘á»ƒ hiá»ƒn thá»‹ text.
try:
    print("ğŸ“¦ Loading resources...")
    
    # Load Index
    index = faiss.read_index(INDEX)
    
    # Load Chunks (Sá»­a lá»—i: Pháº£i load cÃ¡i nÃ y ra biáº¿n global)
    chunks = np.load(CHUNKS, allow_pickle=True)
    
    # Load Mapping
    with open(MAP, "r", encoding="utf-8") as f:
        mapping = f.read().splitlines()
        
    print("âœ” Táº¥t cáº£ dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c load thÃ nh cÃ´ng!")

except Exception as e:
    print(f"âŒ ERROR loading resources: {e}")
    # Náº¿u thiáº¿u file quan trá»ng thÃ¬ dá»«ng chÆ°Æ¡ng trÃ¬nh
    exit()

# 3. Äá»‹nh nghÄ©a hÃ m Search
def semantic_search(question, top_k=3):
    # Encode cÃ¢u há»i
    q_emb = model.encode([question], convert_to_numpy=True)
    
    # Search trong FAISS
    distances, idx = index.search(q_emb, top_k)
    
    results = []
    for i in range(top_k):
        # Láº¥y index thá»±c táº¿
        result_index = idx[0][i]
        
        # Kiá»ƒm tra xem index cÃ³ há»£p lá»‡ khÃ´ng (Ä‘á» phÃ²ng lá»—i out of bound)
        if result_index < len(chunks):
            chunk_text = chunks[result_index]
            file_name = mapping[result_index] if result_index < len(mapping) else "Unknown"
            score = distances[0][i]
            results.append((file_name, chunk_text, float(score)))
    
    return results

# 4. Main Execution (Pháº§n báº¡n muá»‘n thÃªm vÃ o)
# Äáº·t á»Ÿ cuá»‘i file
if __name__ == "__main__":
    print("\n--- TEST SEARCH ---")
    query = "How does TCP handshake work?"
    print(f"ğŸ” Searching for: '{query}'\n")
    
    results = semantic_search(query)
    
    for r in results:
        print("-" * 30)
        print(f"File: {r[0]}")
        print(f"Score: {r[2]:.4f}") # Format sá»‘ tháº­p phÃ¢n cho Ä‘áº¹p
        print(f"Chunk: {r[1][:300]} ...") # In 300 kÃ½ tá»± Ä‘áº§u